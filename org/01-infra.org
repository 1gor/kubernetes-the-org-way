#+title: Cloud Infrastructure Provisioning
#+startup: showeverything

*** Creating the infrastructure

**** Confrm current computing nodes

#+BEGIN_SRC sh :results output code
which gcloud
gcloud compute instances list
#+END_SRC

**** Create a custom network

#+BEGIN_SRC sh :results output code
gcloud compute networks create kubernetes --mode custom
#+END_SRC

#+RESULTS:
#+BEGIN_SRC sh
Created [https://www.googleapis.com/compute/v1/projects/EXAMPLE-99999/global/networks/kubernetes].
NAME        MODE    IPV4_RANGE  GATEWAY_IPV4
kubernetes  custom

Instances on this network will not be reachable until firewall rules
are created. As an example, you can allow all internal traffic between
instances as well as SSH, RDP, and ICMP by running:

$ gcloud compute firewall-rules create <FIREWALL_NAME> --network kubernetes --allow tcp,udp,icmp --source-ranges <IP_RANGE>
$ gcloud compute firewall-rules create <FIREWALL_NAME> --network kubernetes --allow tcp:22,tcp:3389,icmp

#+END_SRC

**** Create a subnet for the Kubernetes cluster

#+BEGIN_SRC sh :results output :exports both
gcloud compute networks subnets create kubernetes \
  --network kubernetes \
  --range 10.240.0.0/24 \
  --region us-central1
#+END_SRC

#+RESULTS:
: NAME        REGION       NETWORK     RANGE
: kubernetes  us-central1  kubernetes  10.240.0.0/24

*** [5/5] Firewalls

**** DONE Allow ping from anywhere:

#+BEGIN_SRC sh :results output  :exports both
gcloud compute firewall-rules create kubernetes-allow-icmp \
  --allow icmp \
  --network kubernetes \
  --source-ranges 0.0.0.0/0 
#+END_SRC

#+RESULTS:
: NAME                   NETWORK     SRC_RANGES  RULES  SRC_TAGS  TARGET_TAGS
: kubernetes-allow-icmp  kubernetes  0.0.0.0/0   icmp

**** DONE Allow both tcp, udp and ping traffic  within the network broadcast domain:

#+BEGIN_SRC sh :results output  :exports both
gcloud compute firewall-rules create kubernetes-allow-internal \
  --allow tcp:0-65535,udp:0-65535,icmp \
  --network kubernetes \
  --source-ranges 10.240.0.0/24
#+END_SRC

:  NAME                       NETWORK     SRC_RANGES     RULES                         SRC_TAGS  TARGET_TAGS 
:  kubernetes-allow-internal  kubernetes  10.240.0.0/24  tcp:0-65535                                          udp:0-65535 icmp 

**** COMMENT Allow remote desktop??

#+BEGIN_SRC sh :results output  :exports both
gcloud compute firewall-rules create kubernetes-allow-rdp \
  --allow tcp:3389 \
  --network kubernetes \
  --source-ranges 0.0.0.0/0
#+END_SRC

**** DONE Allow ssh from anywhere

#+BEGIN_SRC sh :results output  :exports both
gcloud compute firewall-rules create kubernetes-allow-ssh \
  --allow tcp:22 \
  --network kubernetes \
  --source-ranges 0.0.0.0/0
#+END_SRC

#+RESULTS:
: NAME                  NETWORK     SRC_RANGES  RULES   SRC_TAGS  TARGET_TAGS
: kubernetes-allow-ssh  kubernetes  0.0.0.0/0   tcp:22

**** DONE Allow API server port for health check monitoring

The healthchecks will be coming from the =130.211.0.0/22= range in the Google Cloud platform:
https://cloud.google.com/compute/docs/load-balancing/health-checks

#+BEGIN_SRC sh :results output  :exports both
gcloud compute firewall-rules create kubernetes-allow-healthz \
  --allow tcp:8080 \
  --network kubernetes \
  --source-ranges 130.211.0.0/22
#+END_SRC

#+RESULTS:
: NAME                      NETWORK     SRC_RANGES      RULES     SRC_TAGS  TARGET_TAGS
: kubernetes-allow-healthz  kubernetes  130.211.0.0/22  tcp:8080

#+BEGIN_QUOTE
Modify your firewall rules so HTTP(S) traffic can come from your load
balancer. Both incoming traffic and health checks will be be sent to
your Kubernetes service from an IP in the 130.211.0.0/22 range.
#+END_QUOTE

Source: https://cloud.google.com/container-engine/docs/tutorials/http-balancer

**** DONE Allow API server

#+BEGIN_SRC sh :exports both
gcloud compute firewall-rules create kubernetes-allow-api-server \
  --allow tcp:6443 \
  --network kubernetes \
  --source-ranges 0.0.0.0/0
#+END_SRC

| NAME                        | NETWORK    | SRC_RANGES | RULES    | SRC_TAGS | TARGET_TAGS |
| kubernetes-allow-api-server | kubernetes | 0.0.0.0/0  | tcp:6443 |          |             |

**** Confirm

#+BEGIN_SRC sh :results output code
gcloud compute firewall-rules list --filter "network=kubernetes"
#+END_SRC

#+RESULTS:
#+BEGIN_SRC sh
NAME                         NETWORK     SRC_RANGES      RULES                         SRC_TAGS  TARGET_TAGS
kubernetes-allow-api-server  kubernetes  0.0.0.0/0       tcp:6443
kubernetes-allow-healthz     kubernetes  130.211.0.0/22  tcp:8080
kubernetes-allow-icmp        kubernetes  0.0.0.0/0       icmp
kubernetes-allow-internal    kubernetes  10.240.0.0/24   tcp:0-65535,udp:0-65535,icmp
kubernetes-allow-ssh         kubernetes  0.0.0.0/0       tcp:22
#+END_SRC

*** Create the Kubernetes Public IP Address

**** Public IPs required to be able to connect to the API server from Kubernetes

#+BEGIN_SRC sh :results output :exports both
gcloud compute addresses create kubernetes
#+END_SRC

#+RESULTS:
#+begin_example

STDERR: 

For the following addresses:
 - [kubernetes]
choose a region:
 [1] asia-east1
 [2] europe-west1
 [3] us-central1
 [4] us-east1
 [5] us-west1
Please enter your numeric choice:  3
ERROR: (gcloud.compute.addresses.create) Unable to prompt. Specify the [--region] flag.
#+end_example

#+BEGIN_SRC 
Created [https://www.googleapis.com/compute/v1/projects/EXAMPLE-99999/regions/us-central1/addresses/kubernetes].
---
address: AAA.BBB.CCC.100
kind: compute#address
name: kubernetes
region: us-central1
selfLink: https://www.googleapis.com/compute/v1/projects/EXAMPLE-99999/regions/us-central1/addresses/kubernetes
status: RESERVED
#+END_SRC

**** Confirm the address

#+BEGIN_SRC sh :results output :exports both
gcloud compute addresses list kubernetes
#+END_SRC

#+RESULTS:
: NAME        REGION       ADDRESS          STATUS
: kubernetes  us-central1  AAA.BBB.CCC.100  RESERVED

*** Virtual Machine provisioning
 :properties:
 :header-args: :results output code
 :end:

Use VMs running 16.04, we need:

- 3 etcd nodes
- 3 kubernetes controllers/management (node with api servers, controller, scheduler)
- 3 kubernetes workers/minions nodes (kubelet, docker)

**** Static ip mapping

We use static ip mappings for this:

| NAME        | ROLE(S)                                     |          IP |
| etcd0       | etcd                                        | 10.240.0.10 |
| etcd1       | etcd                                        | 10.240.0.11 |
| etcd2       | etcd                                        | 10.240.0.12 |
|             |                                             |         ... |
| controller0 | api-server,replication-controller,scheduler | 10.240.0.20 |
| controller1 | api-server,replication-controller,scheduler | 10.240.0.21 |
| controller2 | api-server,replication-controller,scheduler | 10.240.0.22 |
|             |                                             |         ... |
| worker0     | kubelet,docker                              | 10.240.0.30 |
| worker1     | kubelet,docker                              | 10.240.0.31 |
| worker1     | kubelet,docker                              | 10.240.0.32 |

**** Etcd quorum

Put all within the same availability zone for now...

***** etcd0

#+BEGIN_SRC sh :exports both
gcloud compute instances create etcd0 \
 --boot-disk-size 200GB \
 --can-ip-forward \
 --image ubuntu-1604-xenial-v20160627 \
 --image-project ubuntu-os-cloud \
 --machine-type n1-standard-1 \
 --private-network-ip 10.240.0.10 \
 --zone us-central1-a \
 --subnet kubernetes
#+END_SRC

#+RESULTS:
#+BEGIN_SRC sh
NAME   ZONE           MACHINE_TYPE   PREEMPTIBLE  INTERNAL_IP  EXTERNAL_IP  STATUS
etcd0  us-central1-a  n1-standard-1               10.240.0.10  AAA.BBB.CCC.3  RUNNING
#+END_SRC

***** etcd1

#+BEGIN_SRC sh :exports both
gcloud compute instances create etcd1 \
 --boot-disk-size 200GB \
 --can-ip-forward \
 --image ubuntu-1604-xenial-v20160627 \
 --image-project ubuntu-os-cloud \
 --machine-type n1-standard-1 \
 --private-network-ip 10.240.0.11 \
 --zone us-central1-a \
 --subnet kubernetes
#+END_SRC

#+RESULTS:
#+BEGIN_SRC sh
NAME   ZONE           MACHINE_TYPE   PREEMPTIBLE  INTERNAL_IP  EXTERNAL_IP  STATUS
etcd1  us-central1-a  n1-standard-1               10.240.0.11  AAA.BBB.CCC.4  RUNNING
#+END_SRC

***** etcd2

#+BEGIN_SRC sh :exports both
gcloud compute instances create etcd2 \
 --boot-disk-size 200GB \
 --can-ip-forward \
 --image ubuntu-1604-xenial-v20160627 \
 --image-project ubuntu-os-cloud \
 --machine-type n1-standard-1 \
 --private-network-ip 10.240.0.12 \
 --zone us-central1-a \
 --subnet kubernetes
#+END_SRC

#+RESULTS:
#+BEGIN_SRC sh
NAME   ZONE           MACHINE_TYPE   PREEMPTIBLE  INTERNAL_IP  EXTERNAL_IP  STATUS
etcd2  us-central1-a  n1-standard-1               10.240.0.12  AAA.BBB.CCC.5  RUNNING
#+END_SRC

**** Controllers

***** controller0

#+BEGIN_SRC sh :exports both
gcloud compute instances create controller0 \
 --boot-disk-size 200GB \
 --can-ip-forward \
 --image ubuntu-1604-xenial-v20160627 \
 --image-project ubuntu-os-cloud \
 --machine-type n1-standard-1 \
 --private-network-ip 10.240.0.20 \
 --zone us-central1-a \
 --subnet kubernetes
#+END_SRC

#+RESULTS:
#+BEGIN_SRC sh
NAME         ZONE           MACHINE_TYPE   PREEMPTIBLE  INTERNAL_IP  EXTERNAL_IP  STATUS
controller0  us-central1-a  n1-standard-1               10.240.0.20  AAA.BBB.CCC.1  RUNNING
#+END_SRC

***** controller1

#+BEGIN_SRC sh :exports both
gcloud compute instances create controller1 \
 --boot-disk-size 200GB \
 --can-ip-forward \
 --image ubuntu-1604-xenial-v20160627 \
 --image-project ubuntu-os-cloud \
 --machine-type n1-standard-1 \
 --private-network-ip 10.240.0.21 \
 --zone us-central1-a \
 --subnet kubernetes
#+END_SRC

#+RESULTS:
#+BEGIN_SRC sh
NAME         ZONE           MACHINE_TYPE   PREEMPTIBLE  INTERNAL_IP  EXTERNAL_IP  STATUS
controller1  us-central1-a  n1-standard-1               10.240.0.21  AAA.BBB.CCC.2  RUNNING
#+END_SRC

***** controller2

#+BEGIN_SRC sh :exports both
gcloud compute instances create controller2 \
 --boot-disk-size 200GB \
 --can-ip-forward \
 --image ubuntu-1604-xenial-v20160627 \
 --image-project ubuntu-os-cloud \
 --machine-type n1-standard-1 \
 --private-network-ip 10.240.0.22 \
 --zone us-central1-a \
 --subnet kubernetes
#+END_SRC

#+RESULTS:
#+BEGIN_SRC sh
NAME         ZONE           MACHINE_TYPE   PREEMPTIBLE  INTERNAL_IP  EXTERNAL_IP  STATUS
controller2  us-central1-a  n1-standard-1               10.240.0.22  AAA.BBB.CCC.9  RUNNING
#+END_SRC


**** Workers

***** worker0

#+BEGIN_SRC sh :exports both
gcloud compute instances create worker0 \
 --boot-disk-size 200GB \
 --can-ip-forward \
 --image ubuntu-1604-xenial-v20160627 \
 --image-project ubuntu-os-cloud \
 --machine-type n1-standard-1 \
 --private-network-ip 10.240.0.30 \
 --zone us-central1-a \
 --subnet kubernetes
#+END_SRC

#+RESULTS:
#+BEGIN_SRC sh
NAME     ZONE           MACHINE_TYPE   PREEMPTIBLE  INTERNAL_IP  EXTERNAL_IP  STATUS
worker0  us-central1-a  n1-standard-1               10.240.0.30  AAA.BBB.CCC.6  RUNNING
#+END_SRC

***** worker1

#+BEGIN_SRC sh :exports both
gcloud compute instances create worker1 \
 --boot-disk-size 200GB \
 --can-ip-forward \
 --image ubuntu-1604-xenial-v20160627 \
 --image-project ubuntu-os-cloud \
 --machine-type n1-standard-1 \
 --private-network-ip 10.240.0.31 \
 --zone us-central1-a \
 --subnet kubernetes
#+END_SRC

#+RESULTS:
#+BEGIN_SRC sh
NAME     ZONE           MACHINE_TYPE   PREEMPTIBLE  INTERNAL_IP  EXTERNAL_IP  STATUS
worker1  us-central1-a  n1-standard-1               10.240.0.31  AAA.BBB.CCC.7  RUNNING
#+END_SRC

***** worker2

#+BEGIN_SRC sh :exports both
gcloud compute instances create worker2 \
 --boot-disk-size 200GB \
 --can-ip-forward \
 --image ubuntu-1604-xenial-v20160627 \
 --image-project ubuntu-os-cloud \
 --machine-type n1-standard-1 \
 --private-network-ip 10.240.0.32 \
 --zone us-central1-a \
 --subnet kubernetes
#+END_SRC

#+RESULTS:
#+BEGIN_SRC sh
NAME     ZONE           MACHINE_TYPE   PREEMPTIBLE  INTERNAL_IP  EXTERNAL_IP  STATUS
worker2  us-central1-a  n1-standard-1               10.240.0.32  AAA.BBB.CCC.8  RUNNING
#+END_SRC
